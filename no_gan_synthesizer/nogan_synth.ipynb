{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from modules import utils, model, metrics\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default plot settings\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.core.common.random_state(None)\n",
    "seed = 105\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_data(n_rows:int=1000):\n",
    "#     # column_patterns = [np.random.uniform(1, 72, n_rows),\n",
    "#     #                    np.random.uniform(18, 118, n_rows),\n",
    "#     #                    np.random.uniform(100, 1000, n_rows),\n",
    "                       \n",
    "#     #                    ]\n",
    "#     data_list = [\n",
    "#         np.random.uniform(1, 72, n_rows),\n",
    "#         np.random.uniform(18, 118, n_rows),\n",
    "#         np.random.uniform(100, 1000, n_rows),\n",
    "#         np.random.choice([0,1], size = n_rows, p = [0.25, 0.75])\n",
    "#                 ]\n",
    "#     data_dict = {}\n",
    "#     for i, d in enumerate(data_list):\n",
    "#         data_dict[f\"x{i}\"] = d\n",
    "#     data = pd.DataFrame(data_dict)\n",
    "\n",
    "#     # print(training_data.shape, npdata.shape)\n",
    "#     # print(training_data)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/VincentGranville/Main/main/Telecom.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "features = ['tenure', 'MonthlyCharges', 'TotalCharges','Churn']\n",
    "data = data[features]\n",
    "# for col in data.select_dtypes(include=[object]).columns:\n",
    "#     data[col] = utils.label_encode(data[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = generate_data(6000)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/VincentGranville/Main/main/Telecom.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "features = ['tenure', 'MonthlyCharges', 'TotalCharges','Churn']\n",
    "data = data[features]\n",
    "\n",
    "# data['Churn'] = data['Churn'].map(category_to_integer)\n",
    "data['TotalCharges'].replace(' ', np.nan, inplace=True)\n",
    "data.dropna(subset=['TotalCharges'], inplace=True) # remove missing data\n",
    "arr1 = data['tenure'].to_numpy()\n",
    "arr2 = data['TotalCharges'].to_numpy()\n",
    "arr2 = arr2.astype(float)\n",
    "residues = arr2 - arr1 * np.sum(arr2) / np.sum(arr1) # also try arr2/arr1\n",
    "data['TotalChargeResidues'] = residues\n",
    "\n",
    "print(data.head())\n",
    "print (data.shape)\n",
    "print (data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['tenure', 'MonthlyCharges', 'TotalChargeResidues','Churn']\n",
    "data = data[features]\n",
    "\n",
    "for col in data.select_dtypes(include=[object]).columns:\n",
    "    data[col] = utils.label_encode(data[col])\n",
    "\n",
    "print(data.head())\n",
    "# Split data into Train & Validation sets\n",
    "training_data  = data.sample(frac = 0.5)\n",
    "validation_data = data.drop(training_data.index)\n",
    "# np_tr = np.array(training_data)\n",
    "# np_val = np.array(validation_data)\n",
    "\n",
    "print(f\"Data Shape: {data.shape}\\nTraining Shape: {training_data.shape}\\nValidation Shape: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Student's Success Data\n",
    "# data = utils.get_cleaned_students_data()\n",
    "\n",
    "# # Split data into Train & Validation sets\n",
    "# training_data  = data.sample(frac = 0.5)\n",
    "# validation_data = data.drop(training_data.index)\n",
    "\n",
    "# print(f\"Data Shape: {data.shape}\\nTraining Shape: {training_data.shape}\\nValidation Shape: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scott_bin_size(data):\n",
    "#     n = len(data)\n",
    "#     h = 3.5 * np.std(data) / (n**(1/3))\n",
    "#     k = int(np.ceil((np.max(data) - np.min(data)) / h))\n",
    "#     return k\n",
    "\n",
    "# def freedman_diaconis_bin_size(data):\n",
    "#     n = len(data)\n",
    "#     iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "#     h = 2 * iqr / (n**(1/3))\n",
    "#     k = int(np.ceil((np.max(data) - np.min(data)) / h))\n",
    "#     return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [50, 40, 40, 4]\n",
    "#features = training_data.columns\n",
    "n_synth_rows = len(validation_data)\n",
    "nogan = model.NoGANSynth(data = training_data,\n",
    "                   bins = bins\n",
    "                   )\n",
    "nogan.create_bin_keys()\n",
    "synth_data = nogan.generate_synthetic_data(n_synth_rows)\n",
    "synth_data['Churn'] = synth_data['Churn'].astype('int') \n",
    "synth_data['tenure'] = synth_data['tenure'].astype('int')\n",
    "print(synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_list = []\n",
    "\n",
    "# for skey in nogan.bin_keys:\n",
    "#     bin_list.append((skey, nogan.bin_keys[skey][\"frequency\"]))\n",
    "    \n",
    "# bin_list.sort(key=lambda item: item[1], reverse=True)\n",
    "# bin_list_df = pd.DataFrame(bin_list, columns = [\"key\",\"freq\"])\n",
    "# bin_list_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a sample pandas DataFrame (replace this with your dataset)\n",
    "# data = {\n",
    "#     'variable1': np.random.randn(100),\n",
    "#     'variable2': np.random.randn(100),\n",
    "#     'variable3': np.random.randn(100)\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Function to calculate ECDF for a given dataset\n",
    "# def ecdf(data):\n",
    "#     n = len(data)\n",
    "#     x = np.sort(data)\n",
    "#     y = np.arange(1, n+1) / n\n",
    "#     return x, y\n",
    "\n",
    "# # Create a new figure\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot ECDF for each variable\n",
    "# for col in df.columns:\n",
    "#     x, y = ecdf(df[col])\n",
    "#     plt.plot(x, y, marker='.', linestyle='none', label=col)\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.xlabel('Value')\n",
    "# plt.ylabel('ECDF')\n",
    "# plt.title('Multivariate ECDF')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecdf_validation1 = metrics.compute_ecdf(validation_data, 1000, False)\n",
    "print(\"\")\n",
    "ecdf_validation2 = metrics.compute_ecdf(validation_data, 1000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_max1, ecdf_real1, ecdf_synth1 = metrics.ks_delta(synth_data, ecdf_validation1)\n",
    "ks_max2, ecdf_real2, ecdf_synth2 = metrics.ks_delta(synth_data, ecdf_validation2)\n",
    "ks_max = max(ks_max1, ks_max2)\n",
    "print(f\"ECDF Kolmogorof-Smirnov dist. (synth. vs valid.): {ks_max:6.4f}\")\n",
    "\n",
    "base_ks_max1, ecdf_val1, ecdf_train1 = metrics.ks_delta(training_data, ecdf_validation1)\n",
    "base_ks_max2, ecdf_val2, ecdf_train2 = metrics.ks_delta(training_data, ecdf_validation2)\n",
    "base_ks_max = max(base_ks_max1, base_ks_max2)\n",
    "print(f\"Base ECDF Kolmogorof-Smirnov dist. (train. vs valid.): {base_ks_max:6.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import ks_2samp\n",
    "\n",
    "# # Create two sample pandas DataFrames (replace with your data)\n",
    "# np.random.seed(42)\n",
    "# data1 = pd.DataFrame(np.random.randn(100, 4), columns=['feature1', 'feature2', 'feature3', 'feature4'])\n",
    "# data2 = pd.DataFrame(np.random.randn(100, 4), columns=['feature1', 'feature2', 'feature3', 'feature4'])\n",
    "\n",
    "# # Define the number of permutations for resampling\n",
    "# num_permutations = 1000\n",
    "\n",
    "# # Initialize an array to store permutation KS statistics for each feature\n",
    "# permuted_ks_stats = {col: [] for col in data1.columns}\n",
    "\n",
    "# # Perform permutation testing\n",
    "# for _ in range(num_permutations):\n",
    "#     combined_data = pd.concat([data1, data2])\n",
    "#     combined_data_shuffled = combined_data.sample(frac=1, replace=False, random_state=None)\n",
    "    \n",
    "#     shuffled_data1 = combined_data_shuffled.iloc[:len(data1)]\n",
    "#     shuffled_data2 = combined_data_shuffled.iloc[len(data1):]\n",
    "    \n",
    "#     for col in data1.columns:\n",
    "#         permuted_ks_statistic, _ = ks_2samp(shuffled_data1[col], shuffled_data2[col])\n",
    "#         permuted_ks_stats[col].append(permuted_ks_statistic)\n",
    "\n",
    "# # Calculate the observed KS statistics for each feature\n",
    "# observed_ks_stats = {col: ks_2samp(data1[col], data2[col])[0] for col in data1.columns}\n",
    "\n",
    "# # Calculate p-values for each feature based on the permutation results\n",
    "# p_values = {col: (np.sum(np.array(permuted_ks_stats[col]) >= observed_ks_stats[col]) + 1) / (num_permutations + 1) for col in data1.columns}\n",
    "\n",
    "# # Define the significance level\n",
    "# alpha = 0.05\n",
    "\n",
    "# # Check if any of the p-values are less than the significance level to reject the null hypothesis\n",
    "# features_different = [col for col in p_values if p_values[col] < alpha]\n",
    "\n",
    "# if features_different:\n",
    "#     print(f\"Null hypothesis rejected for features: {', '.join(features_different)}.\")\n",
    "# else:\n",
    "#     print(\"Null hypothesis not rejected. Distributions are not significantly different for any features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"tenure\",\"MonthlyCharges\", \"TotalChargeResidues\", \"Churn\"]\n",
    "def plot_histogram(dfs, dfv, features, title):\n",
    "    num_graphs = len(features) * 2\n",
    "    num_cols = 2\n",
    "    num_rows = num_graphs // num_cols\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 4 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "    for i, ax in enumerate(axes):\n",
    "        column_name = features[i//2]\n",
    "        if i%2 == 0:\n",
    "            sns.histplot(data = dfv, x = column_name, ax = ax)\n",
    "        else:\n",
    "            sns.histplot(data = dfs, x = column_name, ax = ax)\n",
    "    plt.suptitle(title)\n",
    "\n",
    "def plot_scatter(dfs, dfv, features, title):\n",
    "    feature_combinations = list(itertools.combinations(features, 2))\n",
    "    #print(feature_combinations)\n",
    "    num_graphs = len(feature_combinations) * 2\n",
    "    num_cols = 2\n",
    "    num_rows = num_graphs // num_cols\n",
    "    #feature_idx = 0\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 4 * num_rows))\n",
    "    axes = axes.flatten()\n",
    "    for i, ax in enumerate(axes):\n",
    "        x_col = feature_combinations[i // 2][0]\n",
    "        y_col = feature_combinations[i // 2][1]\n",
    "        if i%2 == 0:\n",
    "            sns.scatterplot(data = dfv, x = x_col, y = y_col, ax = ax)\n",
    "        else:\n",
    "            sns.scatterplot(data = dfs, x = x_col, y = y_col, ax = ax)\n",
    "        \n",
    "    plt.suptitle(title)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(synth_data, validation_data, features,\n",
    "               title = \"Histogram Comparisons (Synthetic vs Validation)\") \n",
    "\n",
    "plot_scatter(synth_data, validation_data, features,\n",
    "             title = \"Scatter Plot Comparisons (Synthetic vs Validation) \")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
